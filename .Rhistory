file1_male <- as.vector(file1_data[file1_data$prediction == "male", ]$persons)
file1_female <- as.vector(file1_data[file1_data$prediction == "female", ]$persons)
file2_male <- as.vector(file2_data[file2_data$prediction == "male", ]$persons)
file2_female <- as.vector(file2_data[file2_data$prediction == "female", ]$persons)
a <- length(intersect(file1_male, file2_male))
b <- length(intersect(file1_male, file2_female))
c <- length(intersect(file1_female, file2_male))
d <- length(intersect(file1_female, file2_female))
cont_table <- matrix(c(a, b, c, d), ncol=2, byrow=TRUE)
cont_table
colnames(cont_table) <- c("male","female")
rownames(cont_table) <- c("male","female")
cont_table <- as.table(cont_table)
print("contingency table:")
print(cont_table)
source('~/PredictingReviewHelpfulness/gbm_nullModel.R')
source('~/PredictingReviewHelpfulness/gbm_nullModel.R')
source('~/PredictingReviewHelpfulness/gbm_nullModel.R')
source('~/PredictingReviewHelpfulness/gbm_nullModel.R')
train.accuracy <- cmTrain$overall[["Accuracy"]]
test.accuracy <- cmTest$overall[["Accuracy"]]
train.accuracy
test.accuracy
eval_score <- list(train.accuracy=train.accuracy, test.accuracy=test.accuracy)
eval_score
null.score <- data.frame()
null.score <- rbind(null.score, eval_score)
null.score
source('~/PredictingReviewHelpfulness/gbm_nullModel.R')
source('~/PredictingReviewHelpfulness/gbm_nullModel.R')
source('~/PredictingReviewHelpfulness/gbm_nullModel.R')
cat("\n",final_null_score)
source('~/PredictingReviewHelpfulness/gbm_nullModel.R')
final_null_score
source('~/PredictingReviewHelpfulness/gbm_nullModel.R')
load("/home/mrinal/PredictingReviewHelpfulness/Evaluation/NULL_SCORE_Racing_V50_R0.9.Rdata")
View(null.score)
source('~/PredictingReviewHelpfulness/gbm_nullModel.R')
scoreFile <- paste0("Evaluation/SCORE_", genre,"_V50_R0.9.Rdata")
load(scoreFile)
final_eval_score <- sapply(Filter(is.numeric, comb.score), mean)
source('~/PredictingReviewHelpfulness/gbm_nullModel.R')
source('~/PredictingReviewHelpfulness/gbm_nullModel.R')
scoreFile <- paste0("Evaluation/SCORE_", genre,"_V50_R0.9.Rdata")
load(scoreFile)
final_eval_score <- sapply(Filter(is.numeric, comb.score), mean)
cmTrain$table
tbl <- cmTrain$table
tbl
str(tbl)
actual = c('a','b','c')[runif(100, 1,4)] # actual labels
actual
cont_table <- matrix(c(1750, 52, 0, 0), ncol=2, byrow=TRUE)
cont_table
Reference <- c("No" "Yes")
Reference <- c("No", "Yes")
Prediction <- c("No", "Yes")
colnames(cont_table) <- Reference
rownames(cont_table) <- Prediction
cont_table
table(cont_table)
as.table(cont_table)
#Confusion matrix of the null model for training dataset
null.cmTrain <- as.table(array(c(1750, 52, 0, 0),dim=c(2,2), dimnames=list(c("No","Yes"),c("No","Yes"))))
names(attributes(null.cmTrain)$dimnames) <- c("Reference","Prediction")
null.cmTrain
#Confusion matrix of the null model for training dataset
null.cmTrain <- as.table(array(c(1750, 0, 52, 0),dim=c(2,2), dimnames=list(c("No","Yes"),c("No","Yes"))))
names(attributes(null.cmTrain)$dimnames) <- c("Reference","Prediction")
null.cmTrain
#Confusion matrix of the null model for training dataset
null.cmTrain <- as.table(array(c(1750, 0, 52, 0),dim=c(2,2), dimnames=list(c("No","Yes"),c("No","Yes"))))
names(attributes(null.cmTrain)$dimnames) <- c("Reference","Prediction")
print(null.cmTrain)
#Confusion matrix of the null model from test dataset
null.cmTest <- as.table(array(c(437, 0, 12, 0),dim=c(2,2), dimnames=list(c("No","Yes"),c("No","Yes"))))
names(attributes(null.cmTest)$dimnames) <- c("Reference","Prediction")
print(null.cmTest)
#Confusion matrix of the null model from training dataset
null.cmTrain <- as.table(array(c(1750, 0, 52, 0),dim=c(2,2), dimnames=list(c("No","Yes"),c("No","Yes"))))
names(attributes(null.cmTrain)$dimnames) <- c("Reference","Prediction")
print(null.cmTrain)
#Plot AUC curve
test.pred <- predict(gbmFit, gbm.testX, type="prob")
# Installing and loading required packages
list.of.packages <- c("caret", "data.table","parallel","doMC", "gbm", "e1071", "xgboost", "ROCR", "pROC", "ROSE", "plyr", "binom", "rmarkdown", "knitr")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages, repos="http://cran.rstudio.com/")
library(data.table)
library(parallel)
library(caret)
library(doMC)
library(gbm)
library(e1071)
library(xgboost)
library(ROCR)
library(pROC)
#library(DMwR)
library(ROSE)
library(plyr)
library(binom)
library(rmarkdown)
library(knitr)
#Set path
path = "~/PredictingReviewHelpfulness/"
setwd(path)
#Model Evaluation function
get_eval_score = function(model.fit, trainX, trainY, testX, testY, score, method){
positiveClass <- ifelse(score %in% c(0.05, 0.10), "No", "Yes")
#Evaluation on Training Data
#train.pred <- predict(model.fit, trainX)
train.pred <- predict(model.fit, type = "raw")
train.ref <- trainY#dataTrain$helpful
train.confMatrx <- confusionMatrix(data=train.pred, reference=train.ref, positive=positiveClass, mode = "everything")
train.precision <- train.confMatrx$byClass[["Precision"]]
train.recall <- train.confMatrx$byClass[["Recall"]]
train.sensitivity <- train.confMatrx$byClass[["Sensitivity"]]
train.specificity <- train.confMatrx$byClass[["Specificity"]]
train.accuracy <- train.confMatrx$overall[["Accuracy"]]
train.f1score <- train.confMatrx$byClass[["F1"]]
#Training AUC
train.pred.prob <- predict(model.fit, type = "prob")
pred_vector <- train.pred.prob$Yes
if(positiveClass == "No")
pred_vector <- train.pred.prob$No
ref_vector <- as.factor(ifelse(train.ref == positiveClass, 1, 0)) #make numeric
auc.pred <- prediction(predictions = pred_vector, labels = ref_vector)
auc.tmp <- performance(auc.pred,"auc");
train.auc <- as.numeric(auc.tmp@y.values)
#Evaluation on Test Data
test.pred <- predict(model.fit, testX)
test.ref <- testY#dataTest$helpful
test.confMatrx <- confusionMatrix(data=test.pred, reference=test.ref, positive=positiveClass, mode = "everything")
test.precision <- test.confMatrx$byClass[["Precision"]]
test.recall <- test.confMatrx$byClass[["Recall"]]
test.sensitivity <- test.confMatrx$byClass[["Sensitivity"]]
test.specificity <- test.confMatrx$byClass[["Specificity"]]
test.accuracy <- test.confMatrx$overall[["Accuracy"]]
test.f1score <- test.confMatrx$byClass[["F1"]]
#cat("\n\tF1score: ", test.f1score)
#Test AUC
test.pred.prob <- predict(model.fit, testX, type = "prob")
pred_vector <- test.pred.prob$Yes
if(positiveClass == "No")
pred_vector <- test.pred.prob$No
ref_vector <- as.numeric(ifelse(test.ref == positiveClass, 1, 0)) #make numeric
auc.pred <- prediction(predictions = pred_vector, labels = ref_vector)
auc.tmp <- performance(auc.pred,"auc");
test.auc <- as.numeric(auc.tmp@y.values)
return(list(train.precision=train.precision,
train.recall=train.recall,
train.sensitivity=train.sensitivity,
train.specificity=train.specificity,
train.accuracy=train.accuracy,
train.f1score=train.f1score,
train.auc=train.auc,
test.precision=test.precision,
test.recall=test.recall,
test.sensitivity=test.sensitivity,
test.specificity=test.specificity,
test.accuracy=test.accuracy,
test.f1score=test.f1score,
test.auc=test.auc,
method=method))
}
#Define some variables
genre <- "Racing"
vote_num <- 50
#load dataset
dataFile <- paste0("Dataset/Reviews_", genre ,"_50.Rdata")
if(!file.exists(dataFile) || file.size(dataFile) == 0){
stop("Data file doesn't exist")
}
load(dataFile)
original_dataset <- ds.reviews
# dimensions of dataset
dim(original_dataset)
##Add a helpful column
score_thrsld <- 0.90
original_dataset$helpful <- ifelse(original_dataset$ws.score > score_thrsld, "Yes", "No")
original_dataset$helpful <- as.factor(original_dataset$helpful)
dim.y <- "helpful"
#Load feature file
featureFile <- paste0("Features/FT_", genre, "_V", vote_num, "_R", score_thrsld,"_S25.Rdata")
if(!file.exists(featureFile)){
stop("\nFeature File Not Exists!!")
}
load(featureFile)
#Total number of features
max.dims <- NROW(feature.rank)
print(max.dims)
#Total feature list
dim.x <- feature.rank[order(feature.rank$total)[1:max.dims],]$name
#Exclude tfidf features, we found there are not important at all
dim.x <- grep("^tfidf.*?", dim.x, value = TRUE,  invert = TRUE)
dataset <- original_dataset[, c(dim.x, dim.y), with=F]
#Peek at the Data
head(dataset[,1:5], 5)
#Select number of features
gbm.dim <- 840
#Selec the best subset of the total features
gbm.dim.x <- dim.x[1:gbm.dim]
#Total number of rows in the dataset
n.rows <- NROW(dataset)
#Class Distribution
print(table(dataset$helpful))
#Balance data set with both over and under sampling
n.samplePercent <- 100
n.sampleSize <- ceiling(n.rows * n.samplePercent/100)
balanced_data <- ovun.sample(helpful ~ ., data = dataset, method="both", p=0.5, N=n.sampleSize)$data
print(table(balanced_data$helpful))
#Split dataset
split <- 0.80
trainIndex <- as.vector(createDataPartition(y=balanced_data$helpful, p=split, list=FALSE))
#Get train data
gbm.dataTrain <- balanced_data[trainIndex, c(gbm.dim.x,dim.y), ]
dim(gbm.dataTrain)
#Get test data
gbm.dataTest <- balanced_data[-trainIndex, c(gbm.dim.x,dim.y), ]
dim(gbm.dataTest)
#Split train data
gbm.trainX <- gbm.dataTrain[, gbm.dim.x, ]
gbm.trainY <- as.factor(gbm.dataTrain[, dim.y, ])
#Split test data
gbm.testX <- gbm.dataTest[, gbm.dim.x, ]
gbm.testY <- as.factor(gbm.dataTest[, dim.y, ])
#Remove columns with near zero variance
nearZV <- nearZeroVar(gbm.trainX)
if(length(nearZV) > 0){
gbm.trainX <- gbm.trainX[, -nearZV]
gbm.testX <- gbm.testX[, -nearZV]
}
#Preprocess training Data
preObj <- preProcess(gbm.trainX, method = c("center", "scale"))
gbm.trainX <- predict(preObj, gbm.trainX)
#Preprocess test Data
preObj <- preProcess(gbm.testX, method = c("center", "scale"))
gbm.testX <- predict(preObj, gbm.testX)
#Control parameters
gbm.fitControl = trainControl(method="repeatedcv", #small size -> repeatedcv
number=10, #10-fold cv
repeats=3,
returnResamp = "final",
selectionFunction = "best",
classProbs=TRUE,
summaryFunction=twoClassSummary,
allowParallel = TRUE)
#Train the model
gbmFit <- train(gbm.trainX, gbm.trainY, method="gbm", metric="ROC", trControl=gbm.fitControl, verbose=F)
#Get evaluation score
eval_score <- get_eval_score(gbmFit, gbm.trainX, gbm.trainY, gbm.testX, gbm.testY, score_thrsld, "GBM")
#print(eval_score)
scoreFile <- paste0("Evaluation/SCORE_", genre,"_V50_R0.9.Rdata")
load(scoreFile)
final_eval_score <- sapply(Filter(is.numeric, comb.score), mean)
#Plot AUC curve
test.pred <- predict(gbmFit, gbm.testX, type="prob")
test.ref <- gbm.testY
predob <- prediction(test.pred$Yes, test.ref)
perf <- performance(predob, "tpr", "fpr")
plot(perf, ...)
#Plot AUC curve
test.pred <- predict(gbmFit, gbm.testX, type="prob")
test.ref <- gbm.testY
predob <- prediction(test.pred$Yes, test.ref)
perf <- performance(predob, "tpr", "fpr")
plot(perf)
#Confusion matrix of the null model from training dataset
null.cmTrain <- as.table(array(c(1750, 0, 52, 0),dim=c(2,2), dimnames=list(c("No","Yes"),c("No","Yes"))))
names(attributes(null.cmTrain)$dimnames) <- c("Reference","Prediction")
print(null.cmTrain)
#Confusion matrix of the null model from test dataset
null.cmTest <- as.table(array(c(437, 0, 12, 0),dim=c(2,2), dimnames=list(c("No","Yes"),c("No","Yes"))))
names(attributes(null.cmTest)$dimnames) <- c("Reference","Prediction")
print(null.cmTest)
#Important features
gbmImp <- varImp(gbmFit, scale = TRUE)
impFeatures <- gbmImp$importance
#Sort by overall weight
impFeatures <- impFeatures[order(-impFeatures$Overall), , drop = FALSE]
head(impFeatures, 10)
#Load feature importace file
load(paste0("ImpFeatures/Imp_FT_", genre,"_V50_R0.9.Rdata"))
#Select top 20
top_features <- feature.imp[1:20, ]
top_features$feature <- factor(top_features$feature, levels=unique(top_features$feature))
featurePlot <- ggplot(data=top_features, aes(x=feature, y=mean )) +
geom_bar(stat="identity", position=position_dodge(0.6), fill="steelblue", width=0.6) +
scale_x_discrete(limits = rev(levels(top_features$feature))) +
scale_y_continuous(breaks=c(seq(0,100,10))) +
xlab("Features") + ylab("Feature importance (Relative weight)") + # Set axis labels
theme_bw()+
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
#Horizontal bar plot
featurePlot <- featurePlot + coord_flip()
plot(featurePlot)
# Installing and loading required packages
list.of.packages <- c("caret", "data.table","parallel","doMC", "gbm", "e1071", "xgboost", "ROCR", "pROC", "ROSE", "plyr", "binom", "rmarkdown", "knitr")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages, repos="http://cran.rstudio.com/")
library(data.table)
library(parallel)
library(caret)
library(doMC)
library(gbm)
library(e1071)
library(xgboost)
library(ROCR)
library(pROC)
#library(DMwR)
library(ROSE)
library(plyr)
library(binom)
library(rmarkdown)
library(knitr)
#Set path
path = "~/PredictingReviewHelpfulness/"
setwd(path)
#Model Evaluation function
get_eval_score = function(model.fit, trainX, trainY, testX, testY, score, method){
positiveClass <- ifelse(score %in% c(0.05, 0.10), "No", "Yes")
#Evaluation on Training Data
#train.pred <- predict(model.fit, trainX)
train.pred <- predict(model.fit, type = "raw")
train.ref <- trainY#dataTrain$helpful
train.confMatrx <- confusionMatrix(data=train.pred, reference=train.ref, positive=positiveClass, mode = "everything")
train.precision <- train.confMatrx$byClass[["Precision"]]
train.recall <- train.confMatrx$byClass[["Recall"]]
train.sensitivity <- train.confMatrx$byClass[["Sensitivity"]]
train.specificity <- train.confMatrx$byClass[["Specificity"]]
train.accuracy <- train.confMatrx$overall[["Accuracy"]]
train.f1score <- train.confMatrx$byClass[["F1"]]
#Training AUC
train.pred.prob <- predict(model.fit, type = "prob")
pred_vector <- train.pred.prob$Yes
if(positiveClass == "No")
pred_vector <- train.pred.prob$No
ref_vector <- as.factor(ifelse(train.ref == positiveClass, 1, 0)) #make numeric
auc.pred <- prediction(predictions = pred_vector, labels = ref_vector)
auc.tmp <- performance(auc.pred,"auc");
train.auc <- as.numeric(auc.tmp@y.values)
#Evaluation on Test Data
test.pred <- predict(model.fit, testX)
test.ref <- testY#dataTest$helpful
test.confMatrx <- confusionMatrix(data=test.pred, reference=test.ref, positive=positiveClass, mode = "everything")
test.precision <- test.confMatrx$byClass[["Precision"]]
test.recall <- test.confMatrx$byClass[["Recall"]]
test.sensitivity <- test.confMatrx$byClass[["Sensitivity"]]
test.specificity <- test.confMatrx$byClass[["Specificity"]]
test.accuracy <- test.confMatrx$overall[["Accuracy"]]
test.f1score <- test.confMatrx$byClass[["F1"]]
#cat("\n\tF1score: ", test.f1score)
#Test AUC
test.pred.prob <- predict(model.fit, testX, type = "prob")
pred_vector <- test.pred.prob$Yes
if(positiveClass == "No")
pred_vector <- test.pred.prob$No
ref_vector <- as.numeric(ifelse(test.ref == positiveClass, 1, 0)) #make numeric
auc.pred <- prediction(predictions = pred_vector, labels = ref_vector)
auc.tmp <- performance(auc.pred,"auc");
test.auc <- as.numeric(auc.tmp@y.values)
return(list(train.precision=train.precision,
train.recall=train.recall,
train.sensitivity=train.sensitivity,
train.specificity=train.specificity,
train.accuracy=train.accuracy,
train.f1score=train.f1score,
train.auc=train.auc,
test.precision=test.precision,
test.recall=test.recall,
test.sensitivity=test.sensitivity,
test.specificity=test.specificity,
test.accuracy=test.accuracy,
test.f1score=test.f1score,
test.auc=test.auc,
method=method))
}
#Define some variables
genre <- "Racing"
vote_num <- 50
#load dataset
dataFile <- paste0("Dataset/Reviews_", genre ,"_50.Rdata")
if(!file.exists(dataFile) || file.size(dataFile) == 0){
stop("Data file doesn't exist")
}
load(dataFile)
original_dataset <- ds.reviews
# dimensions of dataset
dim(original_dataset)
##Add a helpful column
score_thrsld <- 0.90
original_dataset$helpful <- ifelse(original_dataset$ws.score > score_thrsld, "Yes", "No")
original_dataset$helpful <- as.factor(original_dataset$helpful)
dim.y <- "helpful"
#Load feature file
featureFile <- paste0("Features/FT_", genre, "_V", vote_num, "_R", score_thrsld,"_S25.Rdata")
if(!file.exists(featureFile)){
stop("\nFeature File Not Exists!!")
}
load(featureFile)
#Total number of features
max.dims <- NROW(feature.rank)
print(max.dims)
#Total feature list
dim.x <- feature.rank[order(feature.rank$total)[1:max.dims],]$name
#Exclude tfidf features, we found there are not important at all
dim.x <- grep("^tfidf.*?", dim.x, value = TRUE,  invert = TRUE)
dataset <- original_dataset[, c(dim.x, dim.y), with=F]
#Peek at the Data
head(dataset[,1:5], 5)
#Select number of features
gbm.dim <- 840
#Selec the best subset of the total features
gbm.dim.x <- dim.x[1:gbm.dim]
#Total number of rows in the dataset
n.rows <- NROW(dataset)
#Class Distribution
print(table(dataset$helpful))
#Balance data set with both over and under sampling
n.samplePercent <- 100
n.sampleSize <- ceiling(n.rows * n.samplePercent/100)
balanced_data <- ovun.sample(helpful ~ ., data = dataset, method="both", p=0.5, N=n.sampleSize)$data
print(table(balanced_data$helpful))
#Split dataset
split <- 0.80
trainIndex <- as.vector(createDataPartition(y=balanced_data$helpful, p=split, list=FALSE))
#Get train data
gbm.dataTrain <- balanced_data[trainIndex, c(gbm.dim.x,dim.y), ]
dim(gbm.dataTrain)
#Get test data
gbm.dataTest <- balanced_data[-trainIndex, c(gbm.dim.x,dim.y), ]
dim(gbm.dataTest)
#Split train data
gbm.trainX <- gbm.dataTrain[, gbm.dim.x, ]
gbm.trainY <- as.factor(gbm.dataTrain[, dim.y, ])
#Split test data
gbm.testX <- gbm.dataTest[, gbm.dim.x, ]
gbm.testY <- as.factor(gbm.dataTest[, dim.y, ])
#Remove columns with near zero variance
nearZV <- nearZeroVar(gbm.trainX)
if(length(nearZV) > 0){
gbm.trainX <- gbm.trainX[, -nearZV]
gbm.testX <- gbm.testX[, -nearZV]
}
#Preprocess training Data
preObj <- preProcess(gbm.trainX, method = c("center", "scale"))
gbm.trainX <- predict(preObj, gbm.trainX)
#Preprocess test Data
preObj <- preProcess(gbm.testX, method = c("center", "scale"))
gbm.testX <- predict(preObj, gbm.testX)
#Control parameters
gbm.fitControl = trainControl(method="repeatedcv", #small size -> repeatedcv
number=10, #10-fold cv
repeats=3,
returnResamp = "final",
selectionFunction = "best",
classProbs=TRUE,
summaryFunction=twoClassSummary,
allowParallel = TRUE)
#Train the model
gbmFit <- train(gbm.trainX, gbm.trainY, method="gbm", metric="ROC", trControl=gbm.fitControl, verbose=F)
#Get evaluation score
eval_score <- get_eval_score(gbmFit, gbm.trainX, gbm.trainY, gbm.testX, gbm.testY, score_thrsld, "GBM")
#print(eval_score)
scoreFile <- paste0("Evaluation/SCORE_", genre,"_V50_R0.9.Rdata")
load(scoreFile)
final_eval_score <- sapply(Filter(is.numeric, comb.score), mean)
#Plot AUC curve
test.pred <- predict(gbmFit, gbm.testX, type="prob")
test.ref <- gbm.testY
predob <- prediction(test.pred$Yes, test.ref)
perf <- performance(predob, "tpr", "fpr")
plot(perf)
#Confusion matrix of the null model from training dataset
null.cmTrain <- as.table(array(c(1750, 0, 52, 0),dim=c(2,2), dimnames=list(c("No","Yes"),c("No","Yes"))))
names(attributes(null.cmTrain)$dimnames) <- c("Reference","Prediction")
print(null.cmTrain)
#Confusion matrix of the null model from test dataset
null.cmTest <- as.table(array(c(437, 0, 12, 0),dim=c(2,2), dimnames=list(c("No","Yes"),c("No","Yes"))))
names(attributes(null.cmTest)$dimnames) <- c("Reference","Prediction")
print(null.cmTest)
#Important features
gbmImp <- varImp(gbmFit, scale = TRUE)
impFeatures <- gbmImp$importance
#Sort by overall weight
impFeatures <- impFeatures[order(-impFeatures$Overall), , drop = FALSE]
head(impFeatures, 10)
#Load feature importace file
load(paste0("ImpFeatures/Imp_FT_", genre,"_V50_R0.9.Rdata"))
#Select top 20
top_features <- feature.imp[1:20, ]
top_features$feature <- factor(top_features$feature, levels=unique(top_features$feature))
featurePlot <- ggplot(data=top_features, aes(x=feature, y=mean )) +
geom_bar(stat="identity", position=position_dodge(0.6), fill="steelblue", width=0.6) +
scale_x_discrete(limits = rev(levels(top_features$feature))) +
scale_y_continuous(breaks=c(seq(0,100,10))) +
xlab("Features") + ylab("Feature importance (Relative weight)") + # Set axis labels
theme_bw()+
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
#Horizontal bar plot
featurePlot <- featurePlot + coord_flip()
plot(featurePlot)
knit_with_parameters('~/PredictingReviewHelpfulness/README.Rmd')
knit_with_parameters('~/PredictingReviewHelpfulness/README.Rmd')
devtools::install_github("rstudio/rmarkdown")
install.packages("devtools", dependencies = TRUE)
install.packages("httr", dependencies = TRUE)
install.packages("devtools", dependencies = TRUE)
install.packages("roxygen2", dependencies = TRUE)
install.packages("xml2", dependencies = TRUE)
devtools::install_github("rstudio/rmarkdown")
unlink('README_cache', recursive = TRUE)
