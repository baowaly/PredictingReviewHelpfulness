---
title: "Predicting the Helpfulness of Racing Game Reviews on the Steam Store"
output: html_notebook
---

##Our goal:
The goal of this research is to construct a model that can accurately predict the helpfulness of game reviews. We can define our problem as follows:

> Given a set of reviews, determine whether the reviews are helpful or not on the basis of review data, reviewer expertise and game information of those reviews.

##Source code explanation:
* __feature_ranking.R:__ Models with a large number of features may cause issues, as irrelevant features may cause confusion, or unimportant features may increase the execution cost. Thus, we needed to select a proper subset of features, and that is why we should rank the features in our dataset by their importance. In this step, we employed the Random Forest algorithm as the Recursive Feature Elimination technique to output a sorted list of features by their importance. To rank the features, we employed 10-fold cross validation taking a (random) sample size 25%. We repeated the whole process 10 times and finally calculated the mean weight of each feature to rank them.

* __gbm_eval.R:__ In our experiment, we employed a GBM (Gradient Boosting Machine) classifier for model training and evaluation. Before applying GBM to train the model, we investigated our dataset and found it imbalanced. To combat this imbalanced dataset, we applied a number of options discussed step by step in later sections.
 
* __gbm_nullModel.R:__ For binary classification null model is built to compare the model performance. In our experiment, we to build up a null model we predict all observation to majority class.

##Model training and evaluation:
```{r setup, include = FALSE}
#Installing and loading required packages

list.of.packages <- c("caret", "data.table","parallel","doMC", "gbm", "e1071", "xgboost", "ROCR", "pROC", "ROSE", "plyr", "binom")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages, repos="http://cran.rstudio.com/")

library(data.table)
library(parallel)
library(caret)
library(doMC)
library(gbm)
library(e1071)
library(xgboost)
library(ROCR)
library(pROC)
#library(DMwR)
library(ROSE)
library(plyr)
library(binom)
```


```{r, echo = FALSE}
#Set path
path = "~/PredictingReviewHelpfulness/"
setwd(path)
```

```{r, echo = FALSE}
#Model Evaluation function
get_eval_score = function(model.fit, trainX, trainY, testX, testY, score, method){
  
  positiveClass <- ifelse(score %in% c(0.05, 0.10), "No", "Yes")
  
  #Evaluation on Training Data
  #train.pred <- predict(model.fit, trainX)
  train.pred <- predict(model.fit, type = "raw")
  train.ref <- trainY#dataTrain$helpful
  train.confMatrx <- confusionMatrix(data=train.pred, reference=train.ref, positive=positiveClass, mode = "everything")
  train.accuracy <- train.confMatrx$overall[["Accuracy"]]
  train.f1score <- train.confMatrx$byClass[["F1"]]
  
  
  #Training AUC
  train.pred.prob <- predict(model.fit, type = "prob")
  pred_vector <- train.pred.prob$Yes
  if(positiveClass == "No")
    pred_vector <- train.pred.prob$No
  ref_vector <- as.factor(ifelse(train.ref == positiveClass, 1, 0)) #make numeric
  auc.pred <- prediction(predictions = pred_vector, labels = ref_vector)
  auc.tmp <- performance(auc.pred,"auc");
  train.auc <- as.numeric(auc.tmp@y.values)
  
  #Evaluation on Test Data
  test.pred <- predict(model.fit, testX)
  test.ref <- testY#dataTest$helpful
  test.confMatrx <- confusionMatrix(data=test.pred, reference=test.ref, positive=positiveClass, mode = "everything")
  test.precision <- test.confMatrx$byClass[["Precision"]]
  test.recall <- test.confMatrx$byClass[["Recall"]]
  test.sensitivity <- test.confMatrx$byClass[["Sensitivity"]]
  test.specificity <- test.confMatrx$byClass[["Specificity"]]
  test.accuracy <- test.confMatrx$overall[["Accuracy"]]
  test.f1score <- test.confMatrx$byClass[["F1"]]
  cat("\n\tF1score: ", test.f1score)
  
  #Test AUC
  test.pred.prob <- predict(model.fit, testX, type = "prob")
  pred_vector <- test.pred.prob$Yes
  if(positiveClass == "No")
    pred_vector <- test.pred.prob$No
  ref_vector <- as.numeric(ifelse(test.ref == positiveClass, 1, 0)) #make numeric
  auc.pred <- prediction(predictions = pred_vector, labels = ref_vector)
  auc.tmp <- performance(auc.pred,"auc");
  test.auc <- as.numeric(auc.tmp@y.values)
  
  return(data.table(train.accuracy=train.accuracy, 
                    train.f1score=train.f1score, 
                    train.auc=train.auc, 
                    test.precision=test.precision, 
                    test.recall=test.recall, 
                    test.sensitivity=test.sensitivity, 
                    test.specificity=test.specificity, 
                    test.accuracy=test.accuracy, 
                    test.f1score=test.f1score, 
                    test.auc=test.auc, 
                    method=method))
}
```

On the steam store there are many game genres e.g. action, racing, survival, rpg etc. In this case, we are investigating the helpfulness of Racing game genre. We have considered those reviews which have at least 50 votes.
```{r}
#Define some variables
genre <- "Racing"
vote_num <- 50
```

###1. Loading Dataset
```{r}
#load dataset 
dataFile <- paste0("Dataset/Reviews_", genre ,"_50.Rdata")
if(!file.exists(dataFile) || file.size(dataFile) == 0){
  stop("Data file doesn't exist")
}
original_dataset <- get(load(dataFile))
```

###2. Summarize Dataset
Now it is time to take a look at the data
```{r}
# dimensions of dataset
dim(original_dataset)
```
You should see `r nrow(original_dataset)` instances and `r ncol(original_dataset)` attributes in the original dataset of `r genre` genre.

